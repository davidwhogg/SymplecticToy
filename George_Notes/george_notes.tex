\documentclass{article}

\input{George_Notes/preamble}

\title{Parametrizing symplectic maps}
\author{@gak}
\date{2025 January}

\begin{document}

\maketitle

\section{Idealized Setup}

Raw data consists of positions and velocities $(x_i,p_i)\in\R^{3\times 3}$ of a certain number $i\in[N]$ of stars, along with auxiliary observations $Q_i\in\R^m$, where $m$ is an arbitrary integer. For now, we assume that all data is given without noise.

Here $\R^{3\times 3}$ has the canonical symplectic structure given by $\omega=dx\wedge dp$.

As an assumption (for either moral or ethical reasons), we believe that there exists a symplectic coordinate change
\begin{align*}
    \Phi:\mqty(x\\p)\mapsto\mqty(J\\\theta)
\end{align*}
such that $Q_i=f(J)$, where $f:\R^3\to\R^m$ is some smooth function. That is, the observations $Q_i$ are  intrinsically 3-dimensional (lie on a 3-dimensional manifold embedded in $\R^m$), and the coordinates $J$ are the `first half' of a symplectic coordinate change. This assumption also implies that the angular variables $\theta$ \textit{are not predictive of} $Q$.

Our first task is then to approximate $\Phi$ using a method that ideally preserves symplectic structure.

\section{Solution Approaches}

\subsection{Generating Functions}

In literature, it is customary to describe symplectic transformations as being generated by a scalar function, called the generating function. This construction is simplifying, since it establishes a correspondence between a higher-dimensional diffeomorphism and a single scalar function; however \gak{(as we maybe discuss in an appendix)} it results in an implicit formulation that is more cumbersome (though by no means prohibitive) to work with computationally in order to parametrize such maps.

In this problem, however, the existence of the additional information captured by $Q$ may work to our advantage, as it provides additional information about the desired parametrization. 

We can always begin by learning (using e.g. smooth, fully connected MLPs) two functions $f_\text{nn}$ and $q_\text{nn}$ that satisfy:

\begin{align*}
    f_\text{nn}(x,p)=\hat{J}\qc q_\text{nn}(\hat{J})=\hat{Q}
\end{align*}

where $f_\text{nn}:\R^6\to\R^3$, $q_\text{nn}:\R^3\to\R^m$, and $\hat{Q}\approx Q$ in some appropriate (say $\ell^2$) norm. At this point, the only condition on $f_\text{nn}$ is that it is smooth, and there is, therefore considerable `unidentifiability' in the variable $J$ (within a diffeomorphism of $\R^3$).

At this point we note that $J$ describes the information encoded in $Q$, $f_\text{nn}$ represents `the first half' of the desired symplectic change of variables $\Phi$. We now want to `complete the transformation', i.e. find a function $g_\text{nn}:\R^6\to\R^3$ such that $\Phi=(f_\text{nn},g_\text{nn})$ is symplectic.

To do so, we will define a generating function (once again modelled by a smooth neural network):
\begin{align*}
    G_\text{nn}:
    \begin{split}
        \R^6&\to\R\\
        (x,\hat{J})&\mapsto \xi
    \end{split}
\end{align*}

which satisfies:

\begin{align*}
    \pdv{G_\text{nn}}{x}=p\qc
    \pdv{G_\text{nn}}{\hat{J}}=\hat{\theta}
\end{align*}

In particular, the first condition is enforced as a constraint, while the second condition yields the desired function $g_\text{nn}$ that `completes' the symplectic transformation. Here we have not placed additional conditions on the variable $\theta$, which may be desirable later on.

The described sequential approach yields an \textit{exact} symplectic transformation by making use of a generating function (whose derivatives can be obtained through automatic differentiation). The implicit information becomes available by suitably handling the provided data on $Q$.

\subsection{Symplectic Maps}

Instead of first extracting the information present in $Q$ and subsequently completing the symplectic transformation, it is possible to attempt to tackle these problems simultaneously by making use of (one of several) universal parametrizations of symplectic maps.

These are usually constructed by first parametrizing near-identity symplectic maps which are then composed to yield a `larger' transformation (sill symplectic!). Under this approach we have two transformations to be learned: $\Phi_\text{nn}:\R^6\to\R^6$ which is symplectic, and $q_\text{nn}:\R^3\to\R^m$ which will be used to enforce that the latent variables $J$ are indeed predictive of $Q$.

We have
\begin{align*}
    \Phi_\text{nn}:\mqty(x\\p)\mapsto\mqty(\hat{J}\\\hat{\theta})\qc q_\text{nn}(\hat{J})=\hat{Q}
\end{align*}

where again we ask that $\hat{Q}\approx Q$.

\subsection{Parametrizations of symplectic maps}

There are two main ways to parametrize $\Phi_\text{nn}$. Broadly, the first generates symplectic transformations by estimating a Hamiltonian (e.g. using a smooth neural network) and then using a symplectic integrator to construct the corresponding map \cite{greydanus2019hamiltonian,bertalan2019learning}. In these architectures, if the integrator is differentiable, it is possible to back-propagate through to the network weights parametrizing the Hamiltonian; otherwise, an adjoint method needs to be used (similar to the Neural-ODE setting \cite{chen2018neuralODE}). While this parametrization yields invertible diffeomorphisms, it does not trivial \gak{check} to obtain the inverse map computationally. \gak{There is likely more work in this subfield over the past 5 years that I am not entirely familiar with.} An alternative approach is offered by triangular symplectic architectures \cite{burby2020fast,jin2020sympnets} based on a characterization outlined in \cite{turaev2002polynomial}. Additional structure can be imposed with respect to symmetry \cite{duruisseaux2023approximation} and Lipschitz control \cite{kevrekidis2024neural}. Importantly, such maps are easily invertible, and so a closed-form inverse (parametrized by a similar neural architecture) is available for such maps. \gak{Some more recent work has been done since last year, I am not yet very familiar with it, but I will try to update.}

\bibliography{George_Notes/george_notes.bib}
\bibliographystyle{ieeetr}

\end{document}